<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Token on Hugo Theme Tailwind Example Site</title><link>https://cuisiting.github.io/tags/token/</link><description>Recent content in Token on Hugo Theme Tailwind Example Site</description><generator>Hugo 0.125.2</generator><language>en</language><copyright>Cuisiting</copyright><lastBuildDate>Sun, 10 Mar 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://cuisiting.github.io/tags/token/index.xml" rel="self" type="application/rss+xml"/><item><title>LLM, What's the Token?</title><link>https://cuisiting.github.io/posts/cuisiting/llm-whatisthetoken/</link><pubDate>Sun, 10 Mar 2024 00:00:00 +0000</pubDate><guid>https://cuisiting.github.io/posts/cuisiting/llm-whatisthetoken/</guid><description>Token vs Word Summary: Summary OpenAi
What are tokens and how to count them? | OpenAI Help Center - 1 token ~= 4 chars in English - 1 token ~= ¾ words - 100 tokens ~= 75 words Or - 1-2 sentence ~= 30 tokens - 1 paragraph ~= 100 tokens - 1,500 words ~= 2048 tokens Why need Token? 在大型语言模型（如GPT系列）中，
&amp;ldquo;Token&amp;quot;通常指的是文本处理的基本单位。
在传统的文本处理中，我们可能会将文本分割成词（words）或者句子作为基本的处理单位。然而，在现代的大型语言模型中，&amp;ldquo;Token&amp;quot;可以是更小的单位，如字（characters）、词根、甚至是词的一部分，这取决于所使用的分词方法（tokenization method）。
分词方法将原始文本分解为一系列的token，这些token随后被模型用于训练和生成文本。这一过程允许模型理解和生成包括多种语言在内的复杂文本，因为它可以捕捉到词汇的细微差别、语法结构和语境意义。</description></item></channel></rss>